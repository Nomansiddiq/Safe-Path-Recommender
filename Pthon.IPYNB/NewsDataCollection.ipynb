{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmLJUaDr2guz"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "site= \"https://www.dawn.com/news/1689795\"\n",
        "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "req = Request(site,headers=hdr)\n",
        "page = urlopen(req)\n",
        "data = BeautifulSoup(page, 'html.parser')\n",
        "#print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# find all the sections with specifiedd class name\n",
        "cards_data = data.find_all('div', attrs={'class', 'flex'})\n",
        "\n",
        "#print(cards_data)\n",
        "\n",
        "\n",
        "for card in cards_data:\n",
        "    title = card.find('div', attrs={'class', 'story__link  '})\n",
        "    date =  card.find('div', attrs={'class', 'timestamp--date'})\n",
        "    #date = card.find('div' , attrs={''})\n",
        "    content = card.find('div', attrs={'class', 'story__content  overflow-hidden    text-4  sm:text-4.5        pt-1  mt-1'})\n",
        "    #content = cards_data.find('p')\n",
        "    #print(title.a.span.text,content.p.text)\n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "6icM_qDh2rKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# create a list to store the data\n",
        "scraped_data = []\n",
        "\n",
        "for card in cards_data:\n",
        "\n",
        "    # initialize the dictionary\n",
        "    card_details = {}\n",
        "\n",
        "    # get the News title\n",
        "    news_title = card.find('div', attrs={'class', 'story__link  '})\n",
        "    #news_title = news_title.a.h2.text\n",
        "    #news_title = news_title.text\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # get the News date \n",
        "    news_date = card.find('div', attrs={'class', 'timestamp--date'})\n",
        "    #news_date = news_date.span.text\n",
        "\n",
        "    # get the News date \n",
        "    news_content = card.find('div', attrs={'class', 'story__content  overflow-hidden    text-4  sm:text-4.5        pt-1  mt-1'})\n",
        "    #news_content = news_content.p.text\n",
        "\n",
        "\n",
        "    # add data to the dictionary\n",
        "    card_details['Headling'] = news_title\n",
        "    card_details['Date'] =     news_date\n",
        "    card_details['Content'] =  news_content\n",
        "\n",
        "\n",
        "    # append the scraped data to the list\n",
        "    scraped_data.append(card_details)\n",
        "\n",
        "# create a data frame from the list of dictionaries\n",
        "dataFrame = pd.DataFrame.from_dict(scraped_data)\n",
        "\n",
        "print(dataFrame)\n",
        "\n",
        "# save the scraped data as CSV file\n",
        "#dataFrame.to_csv('C:\\\\Users\\\\Musad\\\\Desktop\\\\datascience\\\\file.csv', index=False)"
      ],
      "metadata": {
        "id": "CHXBL3yD2sIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce178eba-a956-4b61-d90a-a2940e3592f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Headling  Date Content\n",
            "0      None  None    None\n",
            "1      None  None    None\n",
            "2      None  None    None\n",
            "3      None  None    None\n",
            "4      None  None    None\n",
            "5      None  None    None\n",
            "6      None  None    None\n",
            "7      None  None    None\n",
            "8      None  None    None\n",
            "9      None  None    None\n",
            "10     None  None    None\n",
            "11     None  None    None\n",
            "12     None  None    None\n",
            "13     None  None    None\n",
            "14     None  None    None\n",
            "15     None  None    None\n",
            "16     None  None    None\n",
            "17     None  None    None\n",
            "18     None  None    None\n",
            "19     None  None    None\n",
            "20     None  None    None\n",
            "21     None  None    None\n",
            "22     None  None    None\n",
            "23     None  None    None\n",
            "24     None  None    None\n",
            "25     None  None    None\n",
            "26     None  None    None\n",
            "27     None  None    None\n",
            "28     None  None    None\n",
            "29     None  None    None\n",
            "30     None  None    None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1VCnrHJfDL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}